{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc2cac93",
   "metadata": {},
   "source": [
    "Evaluation of different probability thresholds\n",
    "----------------------------------------------\n",
    "\n",
    "All statistics that we presented up to now rely on `.predict` which outputs\n",
    "the most likely label. We haven’t made use of the probability associated with\n",
    "this prediction, which gives the confidence of the classifier in this\n",
    "prediction. By default, the prediction of a classifier corresponds to a\n",
    "threshold of 0.5 probability in a binary classification problem. Let's build a\n",
    "toy dataset to illustrate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0b326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "common_params = {\n",
    "    \"n_samples\": 10_000,\n",
    "    \"n_features\": 2,\n",
    "    \"n_informative\": 2,\n",
    "    \"n_redundant\": 0,\n",
    "    \"n_classes\": 2,  # binary classification\n",
    "    \"random_state\": 0,\n",
    "}\n",
    "X, y = datasets.make_classification(**common_params, weights=[0.6, 0.4])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=0, test_size=0.02\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1410d7de",
   "metadata": {},
   "source": [
    "We can quickly check the predicted probabilities to belong to either class\n",
    "using a `LogisticRegression`. To ease the visualization we select a subset\n",
    "of `n_plot` samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b8a574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "n_plot = 10\n",
    "classifier = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "proba_predicted = pd.DataFrame(\n",
    "    100 * classifier.predict_proba(X_test), columns=classifier.classes_\n",
    ").round(decimals=1)\n",
    "proba_predicted[:n_plot]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40c9ccc",
   "metadata": {},
   "source": [
    "Probabilites sum to 100%. In the binary case it suffices to retain the\n",
    "probability of belonging to the positive class, here shown as an annotation in\n",
    "the `DecisionBoundaryDisplay`. Notice that setting\n",
    "`response_method=\"predict_proba\"` shows the level curves of the 2D sigmoid\n",
    "(logistic curve)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7557460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "disp = DecisionBoundaryDisplay.from_estimator(\n",
    "    classifier,\n",
    "    X_test,\n",
    "    response_method=\"predict_proba\",\n",
    "    alpha=0.5,\n",
    "    ax=ax,\n",
    ")\n",
    "scatter = disp.ax_.scatter(\n",
    "    X_test[:n_plot, 0], X_test[:n_plot, 1], c=y_test[:n_plot], edgecolor=\"k\"\n",
    ")\n",
    "disp.ax_.set_title(\"This is a plot title\")\n",
    "disp.ax_.legend(*scatter.legend_elements())\n",
    "for i, proba in enumerate(proba_predicted[:n_plot][1]):\n",
    "    disp.ax_.annotate(proba, (X_test[i, 0], X_test[i, 1]), fontsize=\"large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85e7d33",
   "metadata": {},
   "source": [
    "The default decision threshold (0.5) might not be the best threshold that\n",
    "leads to optimal generalization performance of our classifier. One can vary\n",
    "the decision threshold, and therefore the underlying prediction, and compute\n",
    "the same evaluation metrics presented earlier. Usually, the recall and\n",
    "precision are computed and plotted on a graph. Each point on the graph\n",
    "corresponds to a specific decision threshold. Let’s start by computing the\n",
    "precision-recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5a85df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_classifier = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_classifier.fit(X_train, y_train)\n",
    "\n",
    "disp = PrecisionRecallDisplay.from_estimator(\n",
    "    classifier, X_test, y_test, name=\"LogisticRegression\", marker=\"+\"\n",
    ")\n",
    "disp = PrecisionRecallDisplay.from_estimator(\n",
    "    dummy_classifier,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    name=\"chance level\",\n",
    "    color=\"tab:orange\",\n",
    "    linestyle=\"--\",\n",
    "    ax=disp.ax_,\n",
    ")\n",
    "\n",
    "plt.legend(loc=\"lower left\")\n",
    "_ = disp.ax_.set_title(\"Precision-recall curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc3dc8b",
   "metadata": {},
   "source": [
    "Notice that the `DummyClassifier` is a way to define chance level, and\n",
    "its average precision coincides with the prevalence of the positive class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408e7624",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Prevalence of the positive class: {y.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65773f05",
   "metadata": {},
   "source": [
    "On this curve, each blue cross corresponds to a level of probability which we\n",
    "used as a decision threshold. We can see that, by varying this decision\n",
    "threshold, we get different precision vs. recall values.\n",
    "\n",
    "A perfect classifier would have a precision of 1 for all recall values. A\n",
    "metric characterizing the curve is linked to the area under the curve (AUC)\n",
    "and is named average precision (AP). With an ideal classifier, the average\n",
    "precision would be 1.\n",
    "\n",
    "The precision and recall metric focuses on the positive class. However, one\n",
    "might be interested in the compromise between accurately discriminating the\n",
    "positive class and accurately discriminating the negative classes. The\n",
    "statistics used for this are sensitivity and specificity. On the one hand,\n",
    "sensitivity is just another name for recall. On the other hand specificity\n",
    "measures the proportion of correctly classified samples in the negative class\n",
    "defined as: TN / (TN + FP). Similar to the precision-recall curve, sensitivity\n",
    "and specificity are generally plotted as a curve called the Receiver Operating\n",
    "Characteristic (ROC) curve. Below is such a curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceddc775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "\n",
    "disp = RocCurveDisplay.from_estimator(classifier, X_test, y_test, name=\"LogisticRegression\", marker=\"+\")\n",
    "disp = RocCurveDisplay.from_estimator(\n",
    "    dummy_classifier, X_test, y_test, name=\"chance level\", color=\"tab:orange\", linestyle=\"--\", ax=disp.ax_\n",
    ")\n",
    "plt.legend()\n",
    "_ = disp.ax_.set_title(\"ROC AUC curve for the \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8987bcdc",
   "metadata": {},
   "source": [
    "This curve was built using the same principle as the precision-recall curve:\n",
    "we vary the probability threshold for determining \"hard\" prediction and\n",
    "compute the metrics. As with the precision-recall curve, we can compute the\n",
    "area under the ROC (ROC-AUC) to characterize the generalization performance of\n",
    "our classifier. However, it is important to observe that the lower bound of\n",
    "the ROC-AUC is 0.5. Indeed, we show the generalization performance of a dummy\n",
    "classifier (the orange dashed line) to show that even the worst generalization\n",
    "performance obtained will be above this line.\n",
    "\n",
    "Let's do something more realistic: We load the [Forest covertypes\n",
    "dataset](https://scikit-learn.org/stable/datasets/real_world.html#covtype-dataset).\n",
    "The samples in this dataset correspond to 30×30m patches of forest in the US,\n",
    "collected for the task of predicting each patch’s cover type, i.e. the\n",
    "dominant species of tree. There are seven covertypes, making this a multiclass\n",
    "classification problem, but in this case we will only retain two of such\n",
    "classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b139b9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.fetch_covtype(return_X_y=True, as_frame=True)\n",
    "mask = np.isin(y, [1, 2])  # Select two classes\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "# Downsample, to see sampling effects in curves\n",
    "X_reserve, X, y_reserve, y = train_test_split(\n",
    "    X, y, stratify=y, random_state=0, test_size=10_000\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=0, test_size=0.02\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"prevalence of the majority class: {max(y.value_counts())/y.value_counts().sum():.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e06b6bb",
   "metadata": {},
   "source": [
    "In this case we also explore some other models to give a visual intuition of what\n",
    "is a good PR-AUC and ROC-AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825b6ccb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "classifiers = {\n",
    "    \"Excellent\": RandomForestClassifier(n_jobs=-1, random_state=1),\n",
    "    \"Good\": make_pipeline(StandardScaler(), LogisticRegression()),\n",
    "    \"Poor\": GaussianNB(var_smoothing=0.3),\n",
    "    \"Chance\": DummyClassifier(strategy=\"most_frequent\"),\n",
    "}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ba55eb",
   "metadata": {},
   "source": [
    "The precision recall curve can be plotted using the `.predict_proba` for linear\n",
    "models or the `.decision_function` in other cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf05125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes([0.18, 0.15, 0.78, 0.78])\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        y_score = clf.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_score = clf.decision_function(X_test)\n",
    "    prec, recall, _ = precision_recall_curve(y_test, y_score, pos_label=clf.classes_[1])\n",
    "    auc = average_precision_score(y_test, y_score, pos_label=clf.classes_[1])\n",
    "    if name == \"Chance\":\n",
    "        # Use a much bigger test set: we do not want to illustrate\n",
    "        # sampling noise\n",
    "        y_score = clf.predict_proba(X_reserve)[:, 1]\n",
    "        y_score += 0.25 * np.random.random(size=y_score.shape)\n",
    "        prec, recall, _ = precision_recall_curve(\n",
    "            y_reserve, y_score, pos_label=clf.classes_[1]\n",
    "        )\n",
    "        auc = average_precision_score(y_reserve, y_score, pos_label=clf.classes_[1])\n",
    "    plt.plot(recall, prec, label=f\"{name}, AUC={auc:.2f}\")\n",
    "\n",
    "plt.xlabel(\"Recall                  \")\n",
    "plt.text(0.56, 0.0485, \"= TPR or sensitivity\", transform=fig.transFigure, size=7)\n",
    "plt.ylabel(\"Precision         \")\n",
    "plt.text(0.1, 0.6, \"= PPV\", transform=fig.transFigure, size=7, rotation=\"vertical\")\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Precision-recall curves for Forest Covertypes dataset\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e5255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Notice that the class imbalance {np.mean(y_reserve - 1):.4f}\\n\"\n",
    "      f\"equals the chance AUC {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9997fb",
   "metadata": {},
   "source": [
    "We can evaluate the ROC curves of the same set of models using the Forest\n",
    "Covertypes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2629150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes([0.18, 0.15, 0.78, 0.78])\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        y_score = clf.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_score = clf.decision_function(X_test)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_score, pos_label=clf.classes_[1])\n",
    "    auc = roc_auc_score(y_test, y_score)\n",
    "    if name == \"Chance\":\n",
    "        fpr = [0, 1]\n",
    "        tpr = [0, 1]\n",
    "        auc = 0.5\n",
    "    plt.plot(fpr, tpr, label=f\"{name}, AUC={auc:.2f}\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate                           \")\n",
    "plt.text(\n",
    "    0.098,\n",
    "    0.575,\n",
    "    \"= sensitivity or recall\",\n",
    "    transform=fig.transFigure,\n",
    "    size=7,\n",
    "    rotation=\"vertical\",\n",
    ")\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"ROC curves for Forest Covertypes dataset\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
