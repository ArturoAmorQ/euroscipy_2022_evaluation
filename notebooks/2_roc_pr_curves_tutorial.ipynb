{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cc1b9f1",
   "metadata": {},
   "source": [
    "Evaluation of non-thresholded prediction\n",
    "========================================\n",
    "\n",
    "All statistics that we presented up to now rely on `.predict` which outputs\n",
    "the most likely label. We havenâ€™t made use of the probability associated with\n",
    "this prediction, which gives the confidence of the classifier in this\n",
    "prediction. By default, the prediction of a classifier corresponds to a\n",
    "threshold of 0.5 probability in a binary classification problem. Let's build a\n",
    "toy dataset to illustrate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641e3833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "common_params = {\n",
    "    \"n_samples\": 10_000,\n",
    "    \"n_features\": 2,\n",
    "    \"n_informative\": 2,\n",
    "    \"n_redundant\": 0,\n",
    "    \"n_classes\": 2,  # binary classification\n",
    "    \"class_sep\": 0.5,\n",
    "    \"random_state\": 0,\n",
    "}\n",
    "X, y = datasets.make_classification(**common_params, weights=[0.6, 0.4])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=0, test_size=0.02\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236a111d",
   "metadata": {},
   "source": [
    "We can quickly check the predicted probabilities to belong to either class\n",
    "using a `LogisticRegression`. To ease the visualization we select a subset\n",
    "of `n_plot` samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c280cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "n_plot = 10\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "proba_predicted = pd.DataFrame(\n",
    "    classifier.predict_proba(X_test), columns=classifier.classes_\n",
    ").round(decimals=2)\n",
    "proba_predicted[:n_plot]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7105ecb2",
   "metadata": {},
   "source": [
    "Probabilites sum to 1. In the binary case it suffices to retain the\n",
    "probability of belonging to the positive class, here shown as an annotation in\n",
    "the `DecisionBoundaryDisplay`. Notice that setting\n",
    "`response_method=\"predict_proba\"` shows the level curves of the 2D sigmoid\n",
    "(logistic curve)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50eee3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "disp = DecisionBoundaryDisplay.from_estimator(\n",
    "    classifier,\n",
    "    X_test,\n",
    "    response_method=\"predict_proba\",\n",
    "    alpha=0.5,\n",
    "    ax=ax,\n",
    ")\n",
    "scatter = disp.ax_.scatter(\n",
    "    X_test[:n_plot, 0], X_test[:n_plot, 1], c=y_test[:n_plot], edgecolor=\"k\"\n",
    ")\n",
    "disp.ax_.legend(*scatter.legend_elements(), title=\"True class\", loc=\"lower right\")\n",
    "for i, proba in enumerate(proba_predicted[:n_plot][1]):\n",
    "    disp.ax_.annotate(proba, (X_test[i, 0], X_test[i, 1]), fontsize=\"large\")\n",
    "plt.xlim(-2.0, 2.0)\n",
    "plt.ylim(-4.0, 4.0)\n",
    "plt.title(\n",
    "    \"Probability of belonging to the positive class\\n(default decision threshold)\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50e9fb6",
   "metadata": {},
   "source": [
    "Evaluation of different probability thresholds\n",
    "==============================================\n",
    "\n",
    "The default decision threshold (0.5) might not be the best threshold that\n",
    "leads to optimal generalization performance of our classifier. One can vary\n",
    "the decision threshold (and therefore the underlying prediction) and compute\n",
    "some evaluation metrics as presented earlier.\n",
    "\n",
    "Receiver Operating Characteristic curve\n",
    "---------------------------------------\n",
    "\n",
    "One could be interested in the compromise between accurately discriminating\n",
    "both the positive class and the negative classes. The statistics used for this\n",
    "are sensitivity and specificity, which measure the proportion of correctly\n",
    "classified samples per class.\n",
    "\n",
    "Sensitivity and specificity are generally plotted as a curve called the\n",
    "Receiver Operating Characteristic (ROC) curve. Each point on the graph\n",
    "corresponds to a specific decision threshold. Below is such a curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672bf5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_classifier = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_classifier.fit(X_train, y_train)\n",
    "\n",
    "disp = RocCurveDisplay.from_estimator(\n",
    "    classifier, X_test, y_test, name=\"LogisticRegression\", color=\"tab:green\"\n",
    ")\n",
    "disp = RocCurveDisplay.from_estimator(\n",
    "    dummy_classifier,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    name=\"chance level\",\n",
    "    color=\"tab:red\",\n",
    "    ax=disp.ax_,\n",
    ")\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title(\"ROC curve for LogisticRegression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee52054e",
   "metadata": {},
   "source": [
    "ROC curves typically feature true positive rate on the Y axis, and false\n",
    "positive rate on the X axis. This means that the top left corner of the plot\n",
    "is the \"ideal\" point - a false positive rate of zero, and a true positive rate\n",
    "of one. This is not very realistic, but it does mean that a larger area under\n",
    "the curve (AUC) is usually better.\n",
    "\n",
    "We can compute the area under the ROC curve (using `roc_auc_score`) to\n",
    "summarize the generalization performance of a model with a single number, or\n",
    "to compare several models across thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477ac94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "\n",
    "classifiers = {\n",
    "    \"Hist Gradient Boosting\": HistGradientBoostingClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_jobs=-1, random_state=1),\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Chance\": DummyClassifier(strategy=\"most_frequent\"),\n",
    "}\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes([0.18, 0.15, 0.78, 0.78])\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    disp = RocCurveDisplay.from_estimator(clf, X_test, y_test, name=name, ax=ax)\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate                           \")\n",
    "plt.text(\n",
    "    0.098,\n",
    "    0.575,\n",
    "    \"= sensitivity or recall\",\n",
    "    transform=fig.transFigure,\n",
    "    size=7,\n",
    "    rotation=\"vertical\",\n",
    ")\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title(\"ROC curves for several models\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ad064b",
   "metadata": {},
   "source": [
    "It is important to notice that the lower bound of the ROC-AUC is 0.5,\n",
    "corresponding to chance level. Indeed, we show the generalization performance\n",
    "of a dummy classifier (the red line) to show that even the worst\n",
    "generalization performance obtained will be above this line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5ee572",
   "metadata": {},
   "source": [
    "Precision-Recall curves\n",
    "-----------------------\n",
    "\n",
    "As mentioned above, maximizing the ROC curve helps finding a compromise\n",
    "between accurately discriminating both the positive class and the negative\n",
    "classes. If the interest is to focus mainly on the positive class, the\n",
    "precision and recall metrics are more appropriated. Similarly to the ROC\n",
    "curve, each point in the Precision-Recall curve corresponds to a level of\n",
    "probability which we used as a decision threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f353eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes([0.18, 0.15, 0.78, 0.78])\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    disp = PrecisionRecallDisplay.from_estimator(clf, X_test, y_test, name=name, ax=ax)\n",
    "plt.xlabel(\"Recall                  \")\n",
    "plt.text(0.56, 0.0485, \"= TPR or sensitivity\", transform=fig.transFigure, size=7)\n",
    "plt.ylabel(\"Precision         \")\n",
    "plt.text(0.1, 0.6, \"= PPV\", transform=fig.transFigure, size=7, rotation=\"vertical\")\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title(\"Precision-recall curve for LogisticRegression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae2f498",
   "metadata": {},
   "source": [
    "A classifier with no false positives would have a precision of 1 for all\n",
    "recall values. In like manner to the ROC-AUC, the area under the curve can be\n",
    "used to characterize the curve in a single number and is named average\n",
    "precision (AP). With an ideal classifier, the average precision would be 1.\n",
    "\n",
    "In this case, notice that the AP of a `DummyClassifier`, used as baseline to\n",
    "define the chance level, coincides with the prevalence of the positive class.\n",
    "This is analogous to the downside of the accuracy score as shown in the first\n",
    "notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad52061",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevalence = y.mean()\n",
    "print(f\"Prevalence of the positive class: {prevalence:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716bd71c",
   "metadata": {},
   "source": [
    "Let's see the effect of adding umbalance between classes in our set of models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fa483e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_classification(**common_params, weights=[0.83, 0.17])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=0, test_size=0.02\n",
    ")\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes([0.18, 0.15, 0.78, 0.78])\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    disp = PrecisionRecallDisplay.from_estimator(clf, X_test, y_test, name=name, ax=ax)\n",
    "plt.xlabel(\"Recall                  \")\n",
    "plt.text(0.56, 0.0485, \"= TPR or sensitivity\", transform=fig.transFigure, size=7)\n",
    "plt.ylabel(\"Precision         \")\n",
    "plt.text(0.1, 0.6, \"= PPV\", transform=fig.transFigure, size=7, rotation=\"vertical\")\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Precision-recall curve for LogisticRegression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567f6a70",
   "metadata": {},
   "source": [
    "The AP of all models decreased, including the baseline defined by the dummy\n",
    "classifier. Indeed, we confirm that AP does not account for prevalence.\n",
    "\n",
    "Conclusions\n",
    "===========\n",
    "\n",
    "- Consider the prevalence in your target population. It may be that the\n",
    "  prevalence in your testing sample is not representative of that of the\n",
    "  target population. In that case, aside from LR+ and LR-, performance metrics\n",
    "  computed from the testing sample will not be representative of those in the\n",
    "  target population.\n",
    "\n",
    "- Never trust a single summary metric (accuracy, balanced accuracy, ROC-AUC,\n",
    "  etc.), but rather look at all the individual metrics. Understand the\n",
    "  implication of your choices to known the right tradeoff."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
