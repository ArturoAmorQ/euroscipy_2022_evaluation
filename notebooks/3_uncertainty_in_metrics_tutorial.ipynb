{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "604de853",
   "metadata": {},
   "source": [
    "\n",
    "Uncertainty in evaluation metrics for classification\n",
    "====================================================\n",
    "\n",
    "Has it ever happen to you that one of your colleagues claim their model with\n",
    "test score of 0.8001 is better than your model with test score of 0.7998?\n",
    "Maybe they are not aware that model-evaluation procedures should gauge not\n",
    "only the expected generalization performance, but also its variations. As\n",
    "usual, let's build a toy dataset to illustrate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096f78da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "common_params = {\n",
    "    \"n_features\": 2,\n",
    "    \"n_informative\": 2,\n",
    "    \"n_redundant\": 0,\n",
    "    \"n_classes\": 2,  # binary classification\n",
    "    \"random_state\": 0,\n",
    "    \"weights\": [0.55, 0.45],\n",
    "}\n",
    "X, y = make_classification(**common_params, n_samples=400)\n",
    "\n",
    "prevalence = y.mean()\n",
    "print(f\"Percentage of samples in the positive class: {100*prevalence:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f132b7",
   "metadata": {},
   "source": [
    "We are already familiar with using a a train-test split to estimate the\n",
    "generalization performance of a model. By default the `train_test_split` uses\n",
    "`shuffle=True`. Let's see what happens if we set a particular seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615abf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1\n",
    ")\n",
    "classifier = LogisticRegression().fit(X_train, y_train)\n",
    "classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79141bd1",
   "metadata": {},
   "source": [
    "Now let's see what happens when shuffling with a different seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75b8b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "classifier = LogisticRegression().fit(X_train, y_train)\n",
    "classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde0b08e",
   "metadata": {},
   "source": [
    "It seems that 42 is indeed the Ultimate answer to the Question of Life, the\n",
    "Universe, and Everything! Or maybe the score of a model depends on the split:\n",
    " - the train-test proportion;\n",
    " - the representativeness of the elements in each set.\n",
    "\n",
    "A more systematic way of evaluating the generalization performance of a model\n",
    "is through cross-validation, which consists of repeating the split such that\n",
    "the training and testing sets are different for each evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3178a156",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, ShuffleSplit\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "cv = ShuffleSplit(n_splits=250, test_size=0.2)\n",
    "\n",
    "scores = cross_val_score(classifier, X, y, cv=cv)\n",
    "print(\n",
    "    \"The mean cross-validation accuracy is: \"\n",
    "    f\"{scores.mean():.2f} ± {scores.std():.2f}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a601e3",
   "metadata": {},
   "source": [
    "Scores have a variability. A sample probabilistic model gives the distribution\n",
    "of observed error: if the classification rate is p, the observed distribution\n",
    "of correct classifications on a set of size follows a binomial distribution.\n",
    "Let's create a function to easily visualize this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ee9c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "def plot_error_distrib(classifier, X, y, cv=5):\n",
    "\n",
    "    n = len(X)\n",
    "\n",
    "    scores = cross_val_score(classifier, X, y, cv=cv)\n",
    "    distrib = stats.binom(n=n, p=scores.mean())\n",
    "\n",
    "    plt.plot(\n",
    "        np.linspace(0, 1, n),\n",
    "        n * distrib.pmf(np.arange(0, n)),\n",
    "        linewidth=2,\n",
    "        color=\"black\",\n",
    "        label=\"binomial distribution\",\n",
    "    )\n",
    "    sns.histplot(scores, stat=\"density\", label=\"empirical distribution\")\n",
    "    plt.xlim(0, 1)\n",
    "    plt.title(\"Accuracy: \" f\"{scores.mean():.2f} ± {scores.std():.2f}.\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_error_distrib(classifier, X, y, cv=cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fcb32e",
   "metadata": {},
   "source": [
    "The empirical distribution is still broader than the theoretical one. This can\n",
    "be explained by the fact that as we are retraining the model on each fold, it\n",
    "actually fluctuates due the sampling noise in the training data, while the\n",
    "model above only accounts for sampling noise in the test data.\n",
    "\n",
    "The situation does get better with more data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442ddefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(**common_params, n_samples=1_000)\n",
    "plot_error_distrib(classifier, X, y, cv=cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a7b727",
   "metadata": {},
   "source": [
    "Importantly, the standard error of the mean (SEM) across folds is not a good\n",
    "measure of this error, as the different data folds are not independent. For\n",
    "instance, doing many random splits reduces the variance arbitrarily, but does\n",
    "not provide actually new data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c2a788",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = ShuffleSplit(n_splits=10, test_size=0.2)\n",
    "X, y = make_classification(**common_params, n_samples=400)\n",
    "scores = cross_val_score(classifier, X, y, cv=cv)\n",
    "\n",
    "print(\n",
    "    f\"Mean accuracy ± SEM with n_split={cv.get_n_splits()}: \"\n",
    "    f\"{scores.mean():.3f} ± {stats.sem(scores):.3f}.\"\n",
    ")\n",
    "\n",
    "cv = ShuffleSplit(n_splits=100, test_size=0.2)\n",
    "scores = cross_val_score(classifier, X, y, cv=cv)\n",
    "\n",
    "print(\n",
    "    f\"Mean accuracy ± SEM with n_split={cv.get_n_splits()}: \"\n",
    "    f\"{scores.mean():.3f} ± {stats.sem(scores):.3f}.\"\n",
    ")\n",
    "\n",
    "cv = ShuffleSplit(n_splits=500, test_size=0.2)\n",
    "scores = cross_val_score(classifier, X, y, cv=cv)\n",
    "\n",
    "print(\n",
    "    f\"Mean accuracy ± SEM with n_split={cv.get_n_splits()}: \"\n",
    "    f\"{scores.mean():.3f} ± {stats.sem(scores):.3f}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22457177",
   "metadata": {},
   "source": [
    "Indeed, the SEM goes to zero as 1/sqrt{`n_splits`}. Wraping-up:\n",
    "- the more data the better;\n",
    "- the more splits, the more descriptive of the variance is the binomial\n",
    "  distribution, but keep in mind that more splits consume more computing\n",
    "  power;\n",
    "- use std instead of SEM to present your results.\n",
    "\n",
    "Now that we have an intuition on the variability of an evaluation metric, we\n",
    "are ready to apply it to our original Diabetes problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e55603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "diabetes_params = {\n",
    "    \"n_samples\": 10_000,\n",
    "    \"n_features\": 2,\n",
    "    \"n_informative\": 2,\n",
    "    \"n_redundant\": 0,\n",
    "    \"n_classes\": 2,  # binary classification\n",
    "    \"shift\": [4, 6],\n",
    "    \"scale\": [10, 25],\n",
    "    \"random_state\": 0,\n",
    "}\n",
    "X, y = make_classification(**diabetes_params, weights=[0.55, 0.45])\n",
    "\n",
    "X_train, X_plot, y_train, y_plot = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.1, random_state=0\n",
    ")\n",
    "\n",
    "estimator = DecisionTreeClassifier(max_depth=2, random_state=0).fit(X_train, y_train)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "disp = DecisionBoundaryDisplay.from_estimator(\n",
    "    estimator,\n",
    "    X_plot,\n",
    "    response_method=\"predict\",\n",
    "    alpha=0.5,\n",
    "    xlabel=\"age (years)\",\n",
    "    ylabel=\"blood sugar level (mg/dL)\",\n",
    "    ax=ax,\n",
    ")\n",
    "scatter = disp.ax_.scatter(X_plot[:, 0], X_plot[:, 1], c=y_plot, edgecolor=\"k\")\n",
    "disp.ax_.set_title(f\"Diabetes test with prevalence = {y.mean():.2f}\")\n",
    "_ = disp.ax_.legend(*scatter.legend_elements())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cad221",
   "metadata": {},
   "source": [
    "Notice that the decision boundary changed with respect to the first notebook\n",
    "we explored. Let's make a remark: models depend on the prevalence of\n",
    "the data they were trained on. Therefore, all metrics (including likelihood ratios)\n",
    "depend on prevalence as much as the model depends on it. The difference is that\n",
    "likelihood ratios extrapolate through populations of different prevalence for\n",
    "a **fixed model**.\n",
    "\n",
    "Let's compute all the metrics and assez their variability in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277a758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "cv = ShuffleSplit(n_splits=50, test_size=0.2)\n",
    "\n",
    "evaluation = defaultdict(list)\n",
    "scoring_strategies = [\n",
    "    \"accuracy\",\n",
    "    \"balanced_accuracy\",\n",
    "    \"recall\",\n",
    "    \"precision\",\n",
    "    \"matthews_corrcoef\",\n",
    "    # \"positive_likelihood_ratio\",\n",
    "    # \"neg_negative_likelihood_ratio\",\n",
    "]\n",
    "\n",
    "for score_name in scoring_strategies:\n",
    "    scores = cross_val_score(estimator, X, y, cv=cv, scoring=score_name)\n",
    "    evaluation[score_name] = scores\n",
    "\n",
    "evaluation = pd.DataFrame(evaluation).aggregate([\"mean\", \"std\"]).T\n",
    "evaluation[\"mean\"].plot.barh(xerr=evaluation[\"std\"]).set_xlabel(\"score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812bdbd6",
   "metadata": {},
   "source": [
    "Notice that `\"positive_likelihood_ratio\"` is not bounded from above and\n",
    "therefore it can't be directly compared with the other metrics on a single\n",
    "plot. Similarly, the `\"neg_negative_likelihood_ratio\"` has a reversed sign (is\n",
    "negative) to follow the scikit-learn convention for metrics for which a lower\n",
    "score is better.\n",
    "\n",
    "In this case we trained the model on nearly balanced classes. Try changing the\n",
    "prevalence and see how the variance of the metrics depend on data imbalance."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
